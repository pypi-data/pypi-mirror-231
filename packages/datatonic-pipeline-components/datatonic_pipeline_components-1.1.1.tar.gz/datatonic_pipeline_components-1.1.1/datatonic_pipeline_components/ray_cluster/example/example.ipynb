{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example notebook reproduces the official Ray example - [Ray AIR XGBoostTrainer on Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/examples/ml-example.html#) without deploying a Kubernetes Cluster. It demonstrates the seamless utilization of this component for launching Ray tasks in either a distributed or non-distributed manner. \n",
    "\n",
    "The serverless deployment of the Ray cluster brings significant advantages to users, eliminating the complexities of managing Kubernetes Clusters and leading to operational cost savings.Compared to a traditional Kubernetes cluster that runs continuously, incurring costs even during idle periods without active Ray tasks (such as training jobs or predictions), this implementation leverages the serverless nature of Vertex AI. As a result, users are charged only for the resources utilized during each task execution, ensuring a more efficient and cost-effective structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "1. Go to the ray_cluster directory and install the dependencies required for running this notebook\n",
    "2. Restart the notebook to see if the kernel is identified automatically. If not, run `python -m ipykernel install --user --name ray_cluster` and restart the notebook.\n",
    "3. Choose the kernel installed with the name `ray_cluster`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from kfp.components import load_component_from_file\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from kfp import compiler\n",
    "from kfp.dsl import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_pipeline(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    staging_bucket: str,\n",
    "    vertex_pipe_sa: str,\n",
    "    pipeline_func_name: str,\n",
    "    parameter_values: dict = None,\n",
    "    enable_cache=False\n",
    "):\n",
    "    \"\"\"Compile and submit a pipeline job to Vertex\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID running the pipeline\n",
    "        project_location (str): GCP project location where pipeline runs\n",
    "        staging_bucket (str): GCS bucket to host pipeline files\n",
    "        vertex_pipe_sa (str): Vertex service account to run the pipeline\n",
    "        pipeline_func_name (str): The name of pipeline function\n",
    "        parameter_values (dict, optional): The parameters passed to the pipeline function. Defaults to None.\n",
    "        enable_cache (bool, optional):Whether to enable cache or not. Defaults to False.\n",
    "    \"\"\"\n",
    "    compiler.Compiler().compile(pipeline_func_name, package_path=f\"{pipeline_func_name}.json\")\n",
    "    aiplatform.init(\n",
    "        project=project_id,\n",
    "        location=project_location,\n",
    "        staging_bucket=staging_bucket,\n",
    "    )\n",
    "\n",
    "    job = pipeline_jobs.PipelineJob(\n",
    "        display_name=\"dummy_pipeline\",\n",
    "        template_path=f\"{pipeline_func_name}.json\",\n",
    "        parameter_values=parameter_values,\n",
    "        enable_caching=enable_cache,\n",
    "    )\n",
    "\n",
    "    job.submit(\n",
    "        service_account=vertex_pipe_sa,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "project_id = \"\" # GCP project id\n",
    "project_location = \"\" # GCP project location\n",
    "pipeline_staging_path = \"\"  # Vertex pipeline staging path\n",
    "vertex_pipe_sa = \"\" # Vertex pipeline service account\n",
    "\n",
    "ray_train_bucket = \"\" # GCS bucket to host ray model\n",
    "smoke_test = \" --smoke-test --disable-check\"\n",
    "batch_mark_100G = \" --size 100G --disable-check\"\n",
    "ray_script= ( \n",
    "    # Clone ray. If ray is already present, don't clone again.\n",
    "    \"git clone --branch ray-2.6.3 https://github.com/ray-project/ray || true;\"\n",
    "    # Run the benchmark.\n",
    "    \"cd ray/release/air_tests/air_benchmarks/workloads;\"\n",
    "    f\"sed -i 's#/mnt/cluster_storage#{ray_train_bucket}#g' xgboost_benchmark.py;\"\n",
    "    \"cat xgboost_benchmark.py;\"\n",
    "    \"python xgboost_benchmark.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Ray component\n",
    "# from datatonic_pipeline_components.ray_cluster import ray_cluster\n",
    "ray_cluster = load_component_from_file(\"../ray_cluster/component.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Case 1: Launch a non-distributed training task\n",
    "\n",
    "The following example indicates how to use this component to run a non-distributed task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name='ray-non-dist-example')\n",
    "def smoke_test_pipeline():\n",
    "    ray_cluster(\n",
    "        ray_task_cmd=ray_script+smoke_test,\n",
    "        node_config_cmd=\"pip install xgboost xgboost_ray\"\n",
    "    ).set_display_name(\"ray_smoke_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_pipeline(\n",
    "    project_id=project_id,\n",
    "    project_location=project_location,\n",
    "    staging_bucket=pipeline_staging_path,\n",
    "    vertex_pipe_sa=vertex_pipe_sa,\n",
    "    pipeline_func_name=smoke_test_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The [job](https://console.cloud.google.com/vertex-ai/locations/europe-west2/pipelines/runs/ray-non-dist-example-20230821134932?project=dt-pc-dev) succeed with the following metrics\n",
    "\n",
    "- training_time: 29.78 seconds\n",
    "- prediction_time: 19.95 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Case 2: Launch a distirbuted training job\n",
    "\n",
    "The following example shows how to leverage this component to run a distributed training job. Firstly, create a custom training job from this component, where you can specify the machine types, replicas and other computational resource parameters. The output is another component with distributed spec configured. Sebsequently, embed the new component in a vertex pipeline definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ray = create_custom_training_job_from_component(\n",
    "    component_spec=ray_cluster,\n",
    "    display_name=\"ray_cluster\",\n",
    "    machine_type=\"e2-standard-16\",\n",
    "    replica_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name='ray-dist-example-100g')\n",
    "def ray_training_100g():\n",
    "    dist_ray(\n",
    "        project=project_id,\n",
    "        location=project_location,\n",
    "        ray_task_cmd=ray_script+batch_mark_100G,\n",
    "        node_config_cmd=\"pip install xgboost xgboost_ray\"\n",
    "    ).set_display_name(\"ray_100G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_pipeline(\n",
    "    project_id=project_id,\n",
    "    project_location=project_location,\n",
    "    staging_bucket=pipeline_staging_path,\n",
    "    vertex_pipe_sa=vertex_pipe_sa,\n",
    "    pipeline_func_name=ray_training_100g,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The [job](https://console.cloud.google.com/vertex-ai/locations/europe-west2/pipelines/runs/ray-dist-example-100g-20230807152129?project=dt-pc-dev) succeed with the following metrics\n",
    "\n",
    "- training_time: 687.145 seconds\n",
    "- prediction_time: 709.824 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_cluster",
   "language": "python",
   "name": "ray_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
