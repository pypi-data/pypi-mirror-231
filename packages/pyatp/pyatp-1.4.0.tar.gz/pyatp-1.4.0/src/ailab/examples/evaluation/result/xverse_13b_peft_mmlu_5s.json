{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.36,
      "acc_stderr": 0.06857142857142856,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.06857142857142856
    },
    "hendrycksTest-anatomy": {
      "acc": 0.54,
      "acc_stderr": 0.07119963311072637,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.07119963311072637
    },
    "hendrycksTest-astronomy": {
      "acc": 0.56,
      "acc_stderr": 0.07091242083423345,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.07091242083423345
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.62,
      "acc_stderr": 0.06934092056863767,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.06934092056863767
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.48,
      "acc_stderr": 0.0713714056959817,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.0713714056959817
    },
    "hendrycksTest-college_biology": {
      "acc": 0.58,
      "acc_stderr": 0.07050835816716035,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.07050835816716035
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.36,
      "acc_stderr": 0.06857142857142856,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.06857142857142856
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.3,
      "acc_stderr": 0.06546536707079771,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.06546536707079771
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.06663945022680341,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.06663945022680341
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.48,
      "acc_stderr": 0.07137140569598172,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.07137140569598172
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2,
      "acc_stderr": 0.057142857142857155,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.057142857142857155
    },
    "hendrycksTest-computer_security": {
      "acc": 0.68,
      "acc_stderr": 0.06663945022680341,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.06663945022680341
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.5,
      "acc_stderr": 0.07142857142857142,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.07142857142857142
    },
    "hendrycksTest-econometrics": {
      "acc": 0.34,
      "acc_stderr": 0.06767268161329719,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.06767268161329719
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.42,
      "acc_stderr": 0.07050835816716035,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.07050835816716035
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.06266203485560372,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.06266203485560372
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.26,
      "acc_stderr": 0.06266203485560373,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.06266203485560373
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.0654653670707977,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.0654653670707977
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.72,
      "acc_stderr": 0.06414269805898185,
      "acc_norm": 0.72,
      "acc_norm_stderr": 0.06414269805898185
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.36,
      "acc_stderr": 0.06857142857142856,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.06857142857142856
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.56,
      "acc_stderr": 0.07091242083423345,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.07091242083423345
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.66,
      "acc_stderr": 0.06767268161329723,
      "acc_norm": 0.66,
      "acc_norm_stderr": 0.06767268161329723
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.62,
      "acc_stderr": 0.06934092056863767,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.06934092056863767
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.82,
      "acc_stderr": 0.0548839220351387,
      "acc_norm": 0.82,
      "acc_norm_stderr": 0.0548839220351387
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.46,
      "acc_stderr": 0.07119963311072637,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.07119963311072637
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2,
      "acc_stderr": 0.057142857142857155,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.057142857142857155
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.62,
      "acc_stderr": 0.06934092056863767,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.06934092056863767
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.36,
      "acc_stderr": 0.06857142857142856,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.06857142857142856
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.72,
      "acc_stderr": 0.06414269805898186,
      "acc_norm": 0.72,
      "acc_norm_stderr": 0.06414269805898186
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4,
      "acc_stderr": 0.06998542122237651,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.06998542122237651
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6,
      "acc_stderr": 0.06998542122237653,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.06998542122237653
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.8,
      "acc_stderr": 0.05714285714285715,
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.05714285714285715
    },
    "hendrycksTest-human_aging": {
      "acc": 0.66,
      "acc_stderr": 0.06767268161329723,
      "acc_norm": 0.66,
      "acc_norm_stderr": 0.06767268161329723
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.7,
      "acc_stderr": 0.06546536707079771,
      "acc_norm": 0.7,
      "acc_norm_stderr": 0.06546536707079771
    },
    "hendrycksTest-international_law": {
      "acc": 0.76,
      "acc_stderr": 0.0610118757258932,
      "acc_norm": 0.76,
      "acc_norm_stderr": 0.0610118757258932
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.74,
      "acc_stderr": 0.06266203485560373,
      "acc_norm": 0.74,
      "acc_norm_stderr": 0.06266203485560373
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.68,
      "acc_stderr": 0.06663945022680344,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.06663945022680344
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.34,
      "acc_stderr": 0.06767268161329719,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.06767268161329719
    },
    "hendrycksTest-management": {
      "acc": 0.74,
      "acc_stderr": 0.06266203485560376,
      "acc_norm": 0.74,
      "acc_norm_stderr": 0.06266203485560376
    },
    "hendrycksTest-marketing": {
      "acc": 0.88,
      "acc_stderr": 0.0464230765979198,
      "acc_norm": 0.88,
      "acc_norm_stderr": 0.0464230765979198
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.6,
      "acc_stderr": 0.06998542122237653,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.06998542122237653
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.68,
      "acc_stderr": 0.06663945022680341,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.06663945022680341
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.46,
      "acc_stderr": 0.07119963311072637,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.07119963311072637
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.32,
      "acc_stderr": 0.06663945022680341,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.06663945022680341
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6,
      "acc_stderr": 0.06998542122237653,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.06998542122237653
    },
    "hendrycksTest-philosophy": {
      "acc": 0.68,
      "acc_stderr": 0.06663945022680343,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.06663945022680343
    },
    "hendrycksTest-prehistory": {
      "acc": 0.62,
      "acc_stderr": 0.06934092056863767,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.06934092056863767
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.42,
      "acc_stderr": 0.07050835816716035,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.07050835816716035
    },
    "hendrycksTest-professional_law": {
      "acc": 0.44,
      "acc_stderr": 0.07091242083423345,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.07091242083423345
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.52,
      "acc_stderr": 0.0713714056959817,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.0713714056959817
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5,
      "acc_stderr": 0.07142857142857142,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.07142857142857142
    },
    "hendrycksTest-public_relations": {
      "acc": 0.62,
      "acc_stderr": 0.06934092056863767,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.06934092056863767
    },
    "hendrycksTest-security_studies": {
      "acc": 0.56,
      "acc_stderr": 0.07091242083423345,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.07091242083423345
    },
    "hendrycksTest-sociology": {
      "acc": 0.64,
      "acc_stderr": 0.06857142857142856,
      "acc_norm": 0.64,
      "acc_norm_stderr": 0.06857142857142856
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.72,
      "acc_stderr": 0.06414269805898186,
      "acc_norm": 0.72,
      "acc_norm_stderr": 0.06414269805898186
    },
    "hendrycksTest-virology": {
      "acc": 0.46,
      "acc_stderr": 0.07119963311072637,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.07119963311072637
    },
    "hendrycksTest-world_religions": {
      "acc": 0.78,
      "acc_stderr": 0.05917804336345136,
      "acc_norm": 0.78,
      "acc_norm_stderr": 0.05917804336345136
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_xverse_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:04:43.900944",
    "model_name": "xverse_13b"
  }
}