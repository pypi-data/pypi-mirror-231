# -*- coding: utf-8 -*-

from icu import BreakIterator, Locale

from tokenizers import pre_tokenizers


class UnicodeTokenizer:
    def __init__(self, lang="zh"):
        self.lang = lang
        self.word_breaker = BreakIterator.createWordInstance(Locale(lang))
        self.sentence_breaker = BreakIterator.createSentenceInstance(Locale(lang))
        self.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Punctuation(), pre_tokenizers.Split(" ?[^(\\s|[.,!?โฆใ๏ผใเฅคุ])]+", "isolated"), pre_tokenizers.UnicodeScripts()])

    def split_lines(self, text):
        self.sentence_breaker.setText(text)
        parts = []
        p0 = 0
        for p1 in self.sentence_breaker:
            part = text[p0:p1]
            parts.append(part)
            p0 = p1
        return parts

    def tokenize(self, text):
        tokens = []
        lines = self.split_lines(text)
        for line in lines:
            spans = self.pre_tokenizer.pre_tokenize_str(line)
            for span, bound in spans:
                tokens += self.tokenize_line(span)
        return tokens

    def tokenize_line(self, line):
        self.word_breaker.setText(line)
        parts = []
        p0 = 0
        for p1 in self.word_breaker:
            part = line[p0:p1]
            parts.append(part)
            p0 = p1
        return parts


def demo_token():
    doc = [
        "๏กฟ'ใใก[เธเธธเธเธเธฐเธเธฑเธเธเธดเธเธตเนเธเนเธเธเธฒเธเนเธกเธทเนเธญเนเธฃเธเธฐเธฑเธตเธดเนเธทเนเนเธถ]โงpays-g[ran]d-blanc-รฉlevรฉ ยป (็ฝ้ซๅคงๅคๅ)๐็'\x0000๐งญ2022๏ผ๏ผ๏ผ๏ผ\U0010ffff",
        "แแขแแกแซแแตแซแแฎแจแแดโง้ฆๅ8.88่ฎพ็ฝฎ stใart_new_word=True ๅ output=[aรงaรญ]๏ผoutput ๅฐฑๆฏๆ็ป๏กฟ๎ดฐย no such name",
        "็่พๅบเธเธธเธเธเธฐเธเธฑเธเธเธดเธเธตเนเธเนเธเธเธฒเธเนเธกเธทเนเธญเนเธฃเธเธฐํ์น ์์ํด์ผpneumonoultramicroscopicsilicovolcanoconiosis",
        "ํ๋๋ฐ ์นด์ดํฐ๊ฐ ์ด๋์ ์์ด์๊๊ญ๊๊๊จ๊ฆ๊ฒ๊๊๊๊๊๊ท๊ถ๊ูุฃุญูุงุก ุชูุงุฑูู ุชุชุทูุจ ูู [MASK] [PAD] [CLS][SEP]",
        """est ๐ด๐นญ๐ถ๐ดฒ๐ง, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par ยซ pays-grand-blanc-รฉlevรฉ ยป (็ฝ้ซๅคงๅคๅ).""",
        "เธงเธฃเธฃเธเธเธเธฉเนเนเธเนเธเธเธฑเธเธจเธถเธเธฉเธฒเธเธฑเนเธเธเธตเธเธตเนเธซเธเธถเนเธ เนเธฃเธตเธขเธเธชเธฒเธเธฒเธงเธดเธเธขเธฒเธเธฒเธฃเธเธญเธกเธเธดเธงเนเธเธญเธฃเนเนเธฅเธฐเธชเธฒเธฃเธชเธเนเธเธจเธเธเธฐเธงเธดเธเธขเธฒเธจเธฒเธชเธเธฃเนเธเธฃเธฐเธขเธธเธเธเนเนเธฅเธฐเธงเธดเธจเธงเธเธฃเธฃเธกเธจเธฒเธชเธเธฃเนเธญเธขเธนเนเธเธตเนเธกเธซเธฒเธงเธดเธเธขเธฒเธฅเธฑเธขเธเธญเธเนเธเนเธเธงเธดเธเธขเธฒเนเธเธเธซเธเธญเธเธเธฒเธขเธขเธทเธกเธเธทเธเธเธฃเธฑเธเธขเธฒเธเธฃเธซเนเธญเธเธชเธกเธธเธเนเธญเธเธชเธฒเธฃเธชเธฑเธกเธกเธเธฒเธเธญเธกเธเธดเธงเนเธเธญเธฃเนเธเธฑเธเธเธฒเธเธฃเธฐเธเธดเธฉเธเนเธเธฑเธเธเธฒเธฃเธเธฑเธเธเธฒเนเธเธกเนเธกเธงเธเธดเธเธเธฅเธฒเธซเธดเธงเธงเธงเนเธซเธกเธซเธฅเธฑเธเธชเธนเธเธฃเนเธซเธกเนเธชเธเธชเธเธเธเนเธเน",
        "เบชเบปเบกเปเบเบฑเบเบเบฐเปเบเบปเปเบฒเบขเบนเปเบซเบปเบงเบเปเบฃเบปเบกเปเบเบเบเบปเบเบเบณเบเบธเบเบณเบฅเบธเบเบเปเบฒเบเปเบกเบทเบญเบเปเบฅเบฐเบเบฐเบชเบฒเบเบชเบฐเปเบฒเบเบปเบเบเปเบฒเบงเปเบเปเบงเปเบฒเบเบธเบเบชเบตเบญเบฐเบเบธเบเบฐเบขเบฒเปเบเบชเบฐเปเปเบเบฐเบญเบปเบเบเบฑเปเบเปเบเบฑเบเบเบธเบเบเบตเปเบเปเบฒเบเปเบกเบทเบญเบเบเบต เบกเบตเบเบธเบเบเบฒเบเบเบปเบเบชเบณเบเบฑเบเบเบตเปเปเบเบตเบเปเบเปเบเปเบงเบฅเบฒเบเปเปเบกเบฒ เปเบเบฅเบฒเบเบฐเบเบฒเบเบเบญเบเบเบฐเบญเบปเบเบซเบผเบฒเบเบเบปเบ เปเบเบฑเปเบ เบชเบปเบกเปเบเบฑเบเบเบฐเปเบเบปเปเบฒเบเบธเบเบเบปเบเบเบธเบฅเบต, เบเบฐเบเบฒเบเบชเบปเบกเปเบเบฑเบเบเบฐเบเบธเบเบเบฐเบเบญเบเบเปเบฒเบเบธเบฅเบฒเปเบฅเบเบกเบฐเบซเบฒเบฅเบฒเบ เปเบเบฑเบเบเบปเปเบ เปเบเบเบฒเบเบเปเบฒเบเบงเบฑเบเบเบฐเบเบฐเบเบตเบเปเบกเบตเบเบฐเบงเบตเบเบปเบเบชเบณเบเบฑเบ เปเบเบฑเปเบ เปเบเบปเปเบฒเบเปเบฒเบเบณเบกเบฒเบเบดเปเบเบเปเบเบเบฐเปเบเบเบชเบธเบฅเบดเบเบฐเบงเบปเบ เบเบปเบกเบกเบฐเบเบธเบเปเบชเบเบฒเบเบดเบเบฑเบ เบซเบผเบทเปเบเบปเปเบฒเบเปเบฒเบเบธเปเบ เปเบเบดเปเบเปเบเบฑเบเบเบฐเปเบญเบฅเบปเบ เปเบเบฑเบเบเบปเปเบ",
    ]
    tokenizer = UnicodeTokenizer()
    for line in doc:
        print(tokenizer.split_lines(line))
        print(tokenizer.pre_tokenizer.pre_tokenize_str(line))
        print(tokenizer.tokenize_line(line))
        print(tokenizer.tokenize(line))



if __name__ == "__main__":
    demo_token()


