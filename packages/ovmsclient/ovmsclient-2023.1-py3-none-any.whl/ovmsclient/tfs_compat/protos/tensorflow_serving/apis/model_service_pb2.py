# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: ovmsclient/tfs_compat/protos/tensorflow_serving/apis/model_service.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from ovmsclient.tfs_compat.protos.tensorflow_serving.apis import get_model_status_pb2 as ovmsclient_dot_tfs__compat_dot_protos_dot_tensorflow__serving_dot_apis_dot_get__model__status__pb2
from ovmsclient.tfs_compat.protos.tensorflow_serving.apis import model_management_pb2 as ovmsclient_dot_tfs__compat_dot_protos_dot_tensorflow__serving_dot_apis_dot_model__management__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nHovmsclient/tfs_compat/protos/tensorflow_serving/apis/model_service.proto\x12\x12ovmsclient.serving\x1aKovmsclient/tfs_compat/protos/tensorflow_serving/apis/get_model_status.proto\x1aKovmsclient/tfs_compat/protos/tensorflow_serving/apis/model_management.proto2\xe7\x01\n\x0cModelService\x12g\n\x0eGetModelStatus\x12).ovmsclient.serving.GetModelStatusRequest\x1a*.ovmsclient.serving.GetModelStatusResponse\x12n\n\x19HandleReloadConfigRequest\x12\'.ovmsclient.serving.ReloadConfigRequest\x1a(.ovmsclient.serving.ReloadConfigResponseB\x03\xf8\x01\x01\x62\x06proto3')

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'ovmsclient.tfs_compat.protos.tensorflow_serving.apis.model_service_pb2', globals())
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'\370\001\001'
  _MODELSERVICE._serialized_start=251
  _MODELSERVICE._serialized_end=482
# @@protoc_insertion_point(module_scope)
