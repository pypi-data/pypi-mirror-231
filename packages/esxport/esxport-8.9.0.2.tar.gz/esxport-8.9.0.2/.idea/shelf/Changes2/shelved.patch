Index: test/esxport/_prepare_search_query_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Export testing.\"\"\"\nfrom __future__ import annotations\n\nimport string\nfrom random import choice, randint\nfrom typing import TYPE_CHECKING, Any\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom src.esxport import EsXport\nfrom src.exceptions import IndexNotFoundError\nfrom src.strings import index_not_found\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from src.click_opt.cli_options import CliOptions\n    from src.elastic import ElasticsearchClient\n\n\n@patch(\"src.esxport.EsXport._validate_fields\")\nclass TestSearchQuery:\n    \"\"\"Tests that a search query with valid input parameters is successful.\"\"\"\n\n    @staticmethod\n    def random_string(str_len: int = 20) -> str:\n        \"\"\"Generates a random string.\"\"\"\n        characters = string.ascii_letters + string.digits\n        return \"\".join(choice(characters) for _ in range(str_len))\n\n    @staticmethod\n    def random_number(upper: int = 100, lower: int = 10000) -> int:\n        \"\"\"Generates a random number.\"\"\"\n        return randint(upper, lower)\n\n    def test_index(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        random_strings = [self.random_string(10) for _ in range(5)]\n        cli_options.index_prefixes = random_strings\n        indexes = \",\".join(random_strings)\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._prepare_search_query()\n        assert es_export.search_args[\"index\"] == indexes\n\n    def test_all_index(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        cli_options.index_prefixes = [\"_all\", \"invalid_index\"]\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._check_indexes()\n        assert es_export.opts.index_prefixes == [\"_all\"]\n\n    def test_invalid_index(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        cli_options.index_prefixes = [\"invalid_index\"]\n        es_export = EsXport(cli_options, mock_es_client)\n\n        with patch.object(es_export.es_client, \"indices_exists\", return_value=False):\n            with pytest.raises(IndexNotFoundError) as exc_info:\n                es_export._check_indexes()\n\n            msg = index_not_found.format(\"invalid_index\", cli_options.url)\n            assert str(exc_info.value) == msg\n\n    def test_size(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        page_size = randint(100, 9999)\n        cli_options.scroll_size = page_size\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._prepare_search_query()\n        assert es_export.search_args[\"size\"] == page_size\n\n    def test_query(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        expected_query: dict[str, Any] = {\"query\": {\"match_all\": {}}}\n        cli_options.query = expected_query\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._prepare_search_query()\n        assert es_export.search_args[\"body\"] == expected_query\n\n    def test_terminate_after(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        random_max = self.random_number()\n        cli_options.max_results = random_max\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._prepare_search_query()\n        assert es_export.search_args[\"terminate_after\"] == random_max\n\n    def test_sort(\n        self: Self,\n        _: Any,\n        mock_es_client: ElasticsearchClient,\n        cli_options: CliOptions,\n    ) -> None:\n        \"\"\"Arr, matey!.\n\n        Let's test if our search query be successful, with valid input parameters!.\n        \"\"\"\n        random_sort = [{self.random_string(): \"asc\"}, {self.random_string(): \"desc\"}]\n        cli_options.sort = random_sort\n\n        es_export = EsXport(cli_options, mock_es_client)\n        es_export._prepare_search_query()\n        assert es_export.search_args[\"sort\"] == random_sort\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/esxport/_prepare_search_query_test.py b/test/esxport/_prepare_search_query_test.py
--- a/test/esxport/_prepare_search_query_test.py	(revision 58f6a383bbc1e73e2e8230f35db6b39b9d89341b)
+++ b/test/esxport/_prepare_search_query_test.py	(date 1695054796827)
@@ -155,3 +155,20 @@
         es_export = EsXport(cli_options, mock_es_client)
         es_export._prepare_search_query()
         assert es_export.search_args["sort"] == random_sort
+
+    def test_debug_option(
+        self: Self,
+        _: Any,
+        mock_es_client: ElasticsearchClient,
+        cli_options: CliOptions,
+        caplog: pytest.LogCaptureFixture,
+    ) -> None:
+        """Arr, matey!.
+
+        Let's test if our search query be successful, with valid input parameters!.
+        """
+        cli_options.debug = True
+
+        es_export = EsXport(cli_options, mock_es_client)
+        es_export._prepare_search_query()
+        assert caplog.records[0].msg == "Error connecting to %s @ %s : %s / %s"
Index: src/strings.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Common used strings.\"\"\"\nindex_not_found = \"Any of index(es) {} does not exist in {}.\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/strings.py b/src/strings.py
--- a/src/strings.py	(revision 58f6a383bbc1e73e2e8230f35db6b39b9d89341b)
+++ b/src/strings.py	(date 1695054724241)
@@ -1,2 +1,4 @@
 """Common used strings."""
 index_not_found = "Any of index(es) {} does not exist in {}."
+using_indexes = "Using indices: {indexes}."
+using_query = "Using query: {query}."
Index: src/esxport.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Main export module.\"\"\"\nfrom __future__ import annotations\n\nimport contextlib\nimport json\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nfrom elasticsearch.exceptions import ConnectionError\nfrom loguru import logger\nfrom tqdm import tqdm\n\nfrom src.constant import FLUSH_BUFFER, TIMES_TO_TRY\nfrom src.exceptions import FieldNotFoundError, IndexNotFoundError\nfrom src.strings import index_not_found\nfrom src.utils import retry\nfrom src.writer import Writer\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from src.click_opt.cli_options import CliOptions\n    from src.elastic import ElasticsearchClient\n\n\nclass EsXport(object):\n    \"\"\"Main class.\"\"\"\n\n    def __init__(self: Self, opts: CliOptions, es_client: ElasticsearchClient) -> None:\n        self.search_args: dict[str, Any] = {}\n        self.opts = opts\n        self.num_results = 0\n        self.scroll_ids: list[str] = []\n        self.scroll_time = \"30m\"\n        self.tmp_file = f\"{opts.output_file}.tmp\"\n        self.rows_written = 0\n\n        self.es_client = es_client\n\n    @retry(ConnectionError, tries=TIMES_TO_TRY)\n    def _check_indexes(self: Self) -> None:\n        \"\"\"Check if input indexes exist.\"\"\"\n        indexes = self.opts.index_prefixes\n        if \"_all\" in indexes:\n            indexes = [\"_all\"]\n        else:\n            indexes_status = self.es_client.indices_exists(index=indexes)\n            if not indexes_status:\n                msg = index_not_found.format(\", \".join(self.opts.index_prefixes), self.opts.url)\n                raise IndexNotFoundError(\n                    msg,\n                )\n        self.opts.index_prefixes = indexes\n\n    def _validate_fields(self: Self) -> None:\n        all_fields_dict: dict[str, list[str]] = {}\n        indices_names = list(self.opts.index_prefixes)\n        all_expected_fields = self.opts.fields.copy()\n        for sort_query in self.opts.sort:\n            sort_key = next(iter(sort_query.keys()))\n            parts = sort_key.split(\".\")\n            sort_param = parts[0] if len(parts) > 0 else sort_key\n            all_expected_fields.append(sort_param)\n        if \"_all\" in all_expected_fields:\n            all_expected_fields.remove(\"_all\")\n\n        for index in indices_names:\n            response: dict[str, Any] = self.es_client.get_mapping(index=index)\n            all_fields_dict[index] = []\n            for field in response[index][\"mappings\"][\"properties\"]:\n                all_fields_dict[index].append(field)\n        all_es_fields = {value for values_list in all_fields_dict.values() for value in values_list}\n\n        for element in all_expected_fields:\n            if element not in all_es_fields:\n                msg = f\"Fields {element} doesn't exist in any index.\"\n                raise FieldNotFoundError(msg)\n\n    def _prepare_search_query(self: Self) -> None:\n        \"\"\"Prepares search query from input.\"\"\"\n        self.search_args = {\n            \"index\": \",\".join(self.opts.index_prefixes),\n            \"scroll\": self.scroll_time,\n            \"size\": self.opts.scroll_size,\n            \"terminate_after\": self.opts.max_results,\n            \"body\": self.opts.query,\n        }\n        if self.opts.sort:\n            self.search_args[\"sort\"] = self.opts.sort\n\n        if \"_all\" not in self.opts.fields:\n            self.search_args[\"_source_includes\"] = \",\".join(self.opts.fields)\n\n        if self.opts.debug:\n            logger.debug(f'Using these indices: {\", \".join(self.opts.index_prefixes)}.')\n            logger.debug(f\"Query {self.opts.query}\")\n            logger.debug(f'Output field(s): {\", \".join(self.opts.fields)}.')\n            logger.debug(f\"Sorting by: {self.opts.sort}.\")\n\n    @retry(ConnectionError, tries=TIMES_TO_TRY)\n    def next_scroll(self: Self, scroll_id: str) -> Any:\n        \"\"\"Paginate to the next page.\"\"\"\n        return self.es_client.scroll(scroll=self.scroll_time, scroll_id=scroll_id)\n\n    def _write_to_temp_file(self: Self, res: Any) -> None:\n        \"\"\"Write to temp file.\"\"\"\n        hit_list = []\n        total_size = int(min(self.opts.max_results, self.num_results))\n        bar = tqdm(\n            desc=self.tmp_file,\n            total=total_size,\n            unit=\"docs\",\n            colour=\"green\",\n        )\n\n        while self.rows_written != total_size:\n            if res[\"_scroll_id\"] not in self.scroll_ids:\n                self.scroll_ids.append(res[\"_scroll_id\"])\n\n            if not res[\"hits\"][\"hits\"]:\n                logger.info(\n                    f'Scroll[{res[\"_scroll_id\"]}] expired(multiple reads?). Saving loaded data.',\n                )\n                break\n            for hit in res[\"hits\"][\"hits\"]:\n                self.rows_written += 1\n                bar.update(1)\n                hit_list.append(hit)\n                if len(hit_list) == FLUSH_BUFFER:\n                    self._flush_to_file(hit_list)\n                    hit_list = []\n            res = self.next_scroll(res[\"_scroll_id\"])\n        bar.close()\n        self._flush_to_file(hit_list)\n\n    @retry(ConnectionError, tries=TIMES_TO_TRY)\n    def search_query(self: Self) -> Any:\n        \"\"\"Search the index.\"\"\"\n        self._validate_fields()\n        self._prepare_search_query()\n        res = self.es_client.search(**self.search_args)\n        self.num_results = res[\"hits\"][\"total\"][\"value\"]\n\n        logger.info(f\"Found {self.num_results} results.\")\n\n        if self.num_results > 0:\n            self._write_to_temp_file(res)\n\n    def _flush_to_file(self: Self, hit_list: list[dict[str, Any]]) -> None:\n        \"\"\"Flush the search results to a temporary file.\"\"\"\n\n        def add_meta_fields() -> None:\n            if self.opts.meta_fields:\n                for fields in self.opts.meta_fields:\n                    data[fields] = hit.get(fields, None)\n\n        with Path(self.tmp_file).open(mode=\"a\", encoding=\"utf-8\") as tmp_file:\n            for hit in hit_list:\n                data = hit[\"_source\"]\n                data.pop(\"_meta\", None)\n                add_meta_fields()\n                tmp_file.write(json.dumps(data))\n                tmp_file.write(\"\\n\")\n\n    def _clean_scroll_ids(self: Self) -> None:\n        \"\"\"Clear all scroll ids.\"\"\"\n        with contextlib.suppress(Exception):\n            self.es_client.clear_scroll(scroll_id=\"_all\")\n\n    def _extract_headers(self: Self) -> list[str]:\n        \"\"\"Extract CSV headers from the first line of the file.\"\"\"\n        with Path(self.tmp_file).open() as f:\n            first_line = json.loads(f.readline().strip(\"\\n\"))\n            return list(first_line.keys())\n\n    def _export(self: Self) -> None:\n        \"\"\"Export the data.\"\"\"\n        headers = self._extract_headers()\n        kwargs = {\n            \"delimiter\": self.opts.delimiter,\n            \"output_format\": self.opts.format,\n        }\n        Writer.write(\n            headers=headers,\n            total_records=self.rows_written,\n            out_file=self.opts.output_file,\n            **kwargs,\n        )\n\n    def export(self: Self) -> None:\n        \"\"\"Export the data.\"\"\"\n        self._check_indexes()\n        self.search_query()\n        self._clean_scroll_ids()\n        self._export()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/esxport.py b/src/esxport.py
--- a/src/esxport.py	(revision 58f6a383bbc1e73e2e8230f35db6b39b9d89341b)
+++ b/src/esxport.py	(date 1695054635396)
@@ -12,7 +12,7 @@
 
 from src.constant import FLUSH_BUFFER, TIMES_TO_TRY
 from src.exceptions import FieldNotFoundError, IndexNotFoundError
-from src.strings import index_not_found
+from src.strings import index_not_found, using_indexes, using_query
 from src.utils import retry
 from src.writer import Writer
 
@@ -92,8 +92,8 @@
             self.search_args["_source_includes"] = ",".join(self.opts.fields)
 
         if self.opts.debug:
-            logger.debug(f'Using these indices: {", ".join(self.opts.index_prefixes)}.')
-            logger.debug(f"Query {self.opts.query}")
+            logger.debug(using_indexes.format(indexes={", ".join(self.opts.index_prefixes)}))
+            logger.debug(using_query.format(query={self.opts.query}))
             logger.debug(f'Output field(s): {", ".join(self.opts.fields)}.')
             logger.debug(f"Sorting by: {self.opts.sort}.")
 
